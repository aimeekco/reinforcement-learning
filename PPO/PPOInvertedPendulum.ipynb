{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["#pip install gymnasium[mujoco]"],"metadata":{"id":"kc4RRR8Xz4F8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gymnasium as gym\n","import numpy as np\n","import torch\n","from torch import nn, optim\n","from time import sleep\n","from matplotlib.pyplot import plot\n","from IPython.display import clear_output\n","\n","num_envs = 16\n","#base_env = gym.vector.SyncVectorEnv([lambda: gym.make(\"InvertedPendulum-v4\") for _ in range(num_envs)])\n","base_env = gym.vector.SyncVectorEnv([lambda: gym.make(\"InvertedDoublePendulum-v4\") for _ in range(num_envs)])\n","\n","#wrapper to reset environments when episodes are done\n","class AutoResetVecEnv(gym.vector.VectorEnv):\n","    def __init__(self, venv):\n","        self.venv = venv\n","        self.num_envs = venv.num_envs\n","        self.observation_space = venv.observation_space\n","        self.action_space = venv.action_space\n","\n","    def reset(self, **kwargs):\n","        return self.venv.reset(**kwargs)\n","\n","    def step(self, actions):\n","        obs, reward, terminated, truncated, info = self.venv.step(actions)\n","        done_flags = np.logical_or(terminated, truncated)\n","        old_done_flags = done_flags.copy()\n","        if done_flags.any():\n","            new_obs, new_info = self.venv.reset()\n","            obs[done_flags] = new_obs[done_flags]\n","        return obs, reward, old_done_flags, old_done_flags, info\n","\n","    def render(self):\n","        return self.venv.render()\n","\n","    def close(self):\n","        self.venv.close()\n","\n","#apply wrapper to environment\n","env = AutoResetVecEnv(base_env)\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using {device} device\")\n","\n","#policy network class: outputs mean and std of Gaussian action distribution\n","class PolicyNetwork(nn.Module):\n","    def __init__(self, obs_dim, act_dim):\n","        super().__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(obs_dim, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 64),\n","            nn.ReLU()\n","        )\n","        self.mean = nn.Linear(64, act_dim)\n","        self.log_std = nn.Parameter(torch.zeros(act_dim))\n","\n","    def forward(self, x):\n","        x = self.fc(x)\n","        mean = self.mean(x)\n","        std = torch.exp(self.log_std)\n","        return mean, std\n","\n","#value network: estimate state value for advantage estimation\n","class ValueNetwork(nn.Module):\n","    def __init__(self, obs_dim):\n","        super().__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(obs_dim, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 1)\n","        )\n","\n","    def forward(self, x):\n","        return self.fc(x)\n","\n","#get the dimensions of observation and action\n","obs_dim = env.observation_space.shape[1]\n","act_dim = env.action_space.shape[1]\n","\n","pi = PolicyNetwork(obs_dim, act_dim).to(device)\n","V  = ValueNetwork(obs_dim).to(device)\n","optimizer = optim.Adam(list(pi.parameters()) + list(V.parameters()), lr=5e-4)\n","\n","gamma = 0.99\n","lam = 0.95\n","num_updates = 100\n","T = 128\n","n_epochs = 5\n","mini_batch_size = 32\n","entropy_coef = 0.01\n","\n","episode_rewards = np.zeros(num_envs)\n","completed_rewards = []\n","\n","obs, info = env.reset()\n","obs = torch.tensor(obs, dtype=torch.float32, device=device)\n","\n","for update in range(num_updates):\n","    states, actions, rewards, dones, log_probs, values = [], [], [], [], [], []\n","\n","    #loop through time steps\n","    for t in range(T):\n","        state = obs #shape: [num_envs, obs_dim]\n","        with torch.no_grad():\n","            #get action dist and value prediction without updating gradient\n","            mean, std = pi(state) #mean, std: [num_envs, act_dim]\n","            dist = torch.distributions.Normal(mean, std)\n","            action = dist.sample()    #shape: [num_envs, act_dim]\n","            log_prob = dist.log_prob(action).sum(dim=-1)\n","            value = V(state).squeeze(-1)  #shape: [num_envs]\n","\n","        #store states, actions, log_probs, value\n","        #each is a list of T tensors\n","        states.append(state)\n","        actions.append(action)\n","        log_probs.append(log_prob)\n","        values.append(value)\n","\n","        #steps in parallel env\n","        actions_np = action.cpu().numpy()\n","        obs_np, reward, terminated, truncated, info = env.step(actions_np)\n","        done_flags = np.logical_or(terminated, truncated) #shape: [num_envs]\n","\n","        #reward tracking\n","        episode_rewards += reward\n","        for i in range(num_envs):\n","            if done_flags[i]:\n","                completed_rewards.append(episode_rewards[i])\n","                episode_rewards[i] = 0.0\n","\n","        #append reward/done as tensors with shape [num_envs]\n","        rewards.append(torch.tensor(reward, dtype=torch.float32, device=device))\n","        dones.append(torch.tensor(done_flags, dtype=torch.float32, device=device))\n","\n","        #update current obs\n","        obs = torch.tensor(obs_np, dtype=torch.float32, device=device)\n","\n","    #estimate value of final state\n","    with torch.no_grad():\n","        last_value = V(obs).squeeze(-1)\n","    values.append(last_value) #values has length T+1\n","\n","    #convert collected data to tensors\n","    rewards_tensor = torch.stack(rewards, dim=0)\n","    dones_tensor   = torch.stack(dones,   dim=0)\n","    values_tensor  = torch.stack(values,  dim=0)\n","\n","    #initialize GAE with zeros\n","    gae = torch.zeros(num_envs, device=device) #shape: [num_envs]\n","    adv_list = []\n","    #iterate backward through time to compute GAE\n","    for t in reversed(range(T)):\n","        # delta_t = r_t + gamma * V(s_{t+1}) * (1 - done_t) - V(s_t)\n","        delta = rewards_tensor[t] + gamma * values_tensor[t+1] * (1 - dones_tensor[t]) - values_tensor[t]\n","        # gae_t = delta_t + gamma * lambda * (1 - done_t) * gae_{t+1}\n","        gae = delta + gamma * lam * (1 - dones_tensor[t]) * gae\n","        adv_list.insert(0, gae) #insert front to maintain order\n","    advantages = torch.stack(adv_list, dim=0) #final advantage tensor with shape [T, num_envs]\n","    returns = advantages + values_tensor[:-1] # returns = advantages + baseline (value estimates)\n","\n","    #flatten everything to [T*num_envs, ...]\n","    states_flat = torch.cat(states, dim=0)\n","    actions_flat = torch.cat(actions, dim=0)\n","    old_log_probs_flat = torch.cat(log_probs, dim=0)\n","    returns_flat = returns.view(-1)\n","    advantages_flat = advantages.view(-1)\n","\n","\n","    N = T * num_envs #total num samples\n","    for epoch in range(n_epochs):\n","        indices = torch.randperm(N)\n","        for i in range(0, N, mini_batch_size):\n","            idx = indices[i:i+mini_batch_size]\n","            s_batch = states_flat[idx]\n","            a_batch = actions_flat[idx]\n","            old_log_probs_batch = old_log_probs_flat[idx]\n","            ret_batch = returns_flat[idx]\n","            adv_batch = advantages_flat[idx]\n","\n","            #policy distribution for batch\n","            mean, std = pi(s_batch)\n","            dist = torch.distributions.Normal(mean, std)\n","            new_log_probs = dist.log_prob(a_batch).sum(dim=-1)\n","            ratio = torch.exp(new_log_probs - old_log_probs_batch)\n","\n","            #normalize advantages\n","            adv_norm = (adv_batch - adv_batch.mean()) / (adv_batch.std() + 1e-8)\n","\n","            #PPO clipped objective\n","            surr1 = ratio * adv_norm\n","            surr2 = torch.clamp(ratio, 0.8, 1.2) * adv_norm\n","            policy_loss = -torch.min(surr1, surr2).mean()\n","            value_loss = nn.MSELoss()(V(s_batch).squeeze(-1), ret_batch)\n","            #entropy bonus\n","            entropy = dist.entropy().sum(dim=-1).mean()\n","\n","            loss = policy_loss + value_loss - entropy_coef * entropy\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","\n","    if len(completed_rewards) > 0:\n","        running_avg = np.mean(completed_rewards[-10:])\n","    else:\n","        running_avg = 0.0\n","    print(f\"Update {update}, Loss: {loss.item():.3f}, Running Avg Reward (last 10 eps): {running_avg:.1f}\")\n","\n","plot(completed_rewards)"],"metadata":{"id":"m8lDaJvlbYb3"},"execution_count":null,"outputs":[]}]}
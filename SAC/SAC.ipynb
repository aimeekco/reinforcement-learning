{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Nvk37ijY8iuV"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import gym\n","import numpy as np\n","import random\n","from collections import deque\n","\n","#define replay buffer\n","default_buffer_size = 100000\n","class ReplayBuffer:\n","    #deque with max length to automatically drop old data\n","    def __init__(self, max_size=default_buffer_size):\n","        self.buffer = deque(maxlen=max_size)\n","\n","    def push(self, state, action, reward, next_state, done):\n","        self.buffer.append((state, action, reward, next_state, done))\n","\n","    #sampling random minibatches\n","    def sample(self, batch_size):\n","        batch = random.sample(self.buffer, batch_size)\n","        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n","        return state, action, reward, next_state, done\n","\n","    def __len__(self):\n","        return len(self.buffer)\n","\n","#clamp for std (-20,2)\n","LOG_STD_MIN = -20\n","LOG_STD_MAX = 2\n","#define actor network (Gaussian distribution output)\n","class Actor(nn.Module):\n","    def __init__(self, num_inputs, num_actions, hidden_size=256):\n","        super(Actor, self).__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(num_inputs, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.ReLU()\n","        )\n","        #output layers for mean and std\n","        self.mean = nn.Linear(hidden_size, num_actions)\n","        self.log_std = nn.Linear(hidden_size, num_actions)\n","\n","    #use clamp to make sure distribution is in range\n","    def forward(self, state):\n","        x = self.net(state)\n","        mean = self.mean(x)\n","        log_std = self.log_std(x).clamp(LOG_STD_MIN, LOG_STD_MAX)\n","        std = log_std.exp() #convert log(std) to std\n","        return mean, std\n","\n","    #tanh squashing: produce log prob within (-1,1)\n","    def sample(self, state):\n","        mean, std = self(state)\n","        normal = torch.distributions.Normal(mean, std)\n","        x_t = normal.rsample() #rsample allows us to backpropagate\n","        y_t = torch.tanh(x_t) #squash\n","        action = y_t\n","        #compute log probability with correction for tanh\n","        log_prob = normal.log_prob(x_t) - torch.log(1 - y_t.pow(2) + 1e-6) #add small number to prevent 0\n","        log_prob = log_prob.sum(1, keepdim=True)\n","        return action, log_prob\n","\n","#define critic network\n","class Critic(nn.Module):\n","    def __init__(self, num_inputs, num_actions, hidden_size=256):\n","        super(Critic, self).__init__()\n","        self.q1_net = nn.Sequential(\n","            nn.Linear(num_inputs + num_actions, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, 1)\n","        )\n","        self.q2_net = nn.Sequential(\n","            nn.Linear(num_inputs + num_actions, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, 1)\n","        )\n","\n","    def forward(self, state, action):\n","        sa = torch.cat([state, action], dim=1)\n","        q1 = self.q1_net(sa)\n","        q2 = self.q2_net(sa)\n","        return q1, q2\n","\n","#define agent to instantiate actor/critic networks and other hyperparameters\n","class SACAgent:\n","    def __init__(self, env, gamma=0.99, tau=0.005, target_entropy=None,\n","                 lr=3e-4, buffer_size=default_buffer_size, batch_size=64):\n","        self.env = env\n","        self.gamma = gamma\n","        self.tau = tau\n","        self.batch_size = batch_size\n","\n","        #get observation and action space dimensions\n","        obs_dim = env.observation_space.shape[0]\n","        action_dim = env.action_space.shape[0]\n","\n","        #we instantiate two actor/critic networks each to prevent overestimation\n","        self.actor = Actor(obs_dim, action_dim).to(device)\n","        self.actor_optim = optim.Adam(self.actor.parameters(), lr=lr)\n","\n","        self.critic = Critic(obs_dim, action_dim).to(device)\n","        self.critic_optim = optim.Adam(self.critic.parameters(), lr=lr)\n","        self.critic_target = Critic(obs_dim, action_dim).to(device)\n","        #copy initial weights to target network\n","        self.critic_target.load_state_dict(self.critic.state_dict())\n","\n","        if target_entropy is None:\n","            self.target_entropy = -action_dim #entropy has neg dimension of the action space\n","        else:\n","            self.target_entropy = target_entropy\n","        #log alpha param for entropy coefficient\n","        self.log_alpha = torch.tensor(np.log(0.2), requires_grad=True, device=device)\n","        self.alpha_optim = optim.Adam([self.log_alpha], lr=lr)\n","\n","        self.buffer = ReplayBuffer(buffer_size)\n","\n","    #getter method for alpha\n","    @property\n","    def alpha(self):\n","        return self.log_alpha.exp() #exp to ensure >0\n","\n","    def select_action(self, state, evaluate=False):\n","        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n","        if evaluate:\n","            mean, _ = self.actor(state)\n","            action = torch.tanh(mean) #also tanh squash whenever we sample an action\n","            return action.cpu().detach().numpy()[0]\n","        else:\n","            action, _ = self.actor.sample(state)\n","            return action.cpu().detach().numpy()[0]\n","\n","    #soft actor/critic updates\n","    def update(self):\n","        #only update if we have enough samples\n","        if len(self.buffer) < self.batch_size:\n","            return\n","        state, action, reward, next_state, done = self.buffer.sample(self.batch_size)\n","        state = torch.FloatTensor(state).to(device)\n","        action = torch.FloatTensor(action).to(device)\n","        reward = torch.FloatTensor(reward).unsqueeze(1).to(device)\n","        next_state = torch.FloatTensor(next_state).to(device)\n","        done = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(device)\n","\n","        #compute target q-value\n","        with torch.no_grad():\n","            next_action, next_log_prob = self.actor.sample(next_state)\n","            q1_next, q2_next = self.critic_target(next_state, next_action)\n","            q_next = torch.min(q1_next, q2_next) - self.alpha.detach() * next_log_prob #take min of estimated q-values\n","            q_target = reward + (1 - done) * self.gamma * q_next #standard update with chosen q-value\n","\n","        #critic update using MSE loss\n","        q1, q2 = self.critic(state, action)\n","        critic_loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target) #minimize loss of both function at once\n","        self.critic_optim.zero_grad()\n","        critic_loss.backward()\n","        self.critic_optim.step()\n","\n","        #actor update: maximize total reward and expected entropy\n","        action_new, log_prob_new = self.actor.sample(state)\n","        q1_new, q2_new = self.critic(state, action_new)\n","        q_new = torch.min(q1_new, q2_new) #take min of estimated q-values\n","        actor_loss = (self.alpha * log_prob_new - q_new).mean()\n","        self.actor_optim.zero_grad()\n","        actor_loss.backward()\n","        self.actor_optim.step()\n","\n","        #alpha loss to tune entropy coefficient\n","        alpha_loss = -(self.log_alpha * (log_prob_new + self.target_entropy).detach()).mean()\n","        self.alpha_optim.zero_grad()\n","        alpha_loss.backward()\n","        self.alpha_optim.step()\n","\n","        #soft updating target critic parameters\n","        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n","            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n","\n","    #add everything to buffer\n","    def store_transition(self, state, action, reward, next_state, done):\n","        self.buffer.push(state, action, reward, next_state, done)\n","\n","#main training loop\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","def train(env_name='Pendulum-v1', max_episodes=200, max_steps=200):\n","    #instantiate environment and agent\n","    env = gym.make(env_name)\n","    agent = SACAgent(env)\n","    total_rewards = []\n","\n","    for episode in range(max_episodes):\n","        state = env.reset()[0]\n","        episode_reward = 0\n","        for step in range(max_steps):\n","            #select and execute an action\n","            action = agent.select_action(state)\n","            next_state, reward, done, truncated, _ = env.step(action) #take step in environment\n","            #store transition and update networks\n","            agent.store_transition(state, action, reward, next_state, done)\n","            agent.update()\n","            state = next_state\n","            episode_reward += reward\n","            if done:\n","                break\n","        #periodic reward reporting\n","        total_rewards.append(episode_reward)\n","        if (episode + 1) % 10 == 0:\n","            avg_reward = np.mean(total_rewards[-10:])\n","            print(f\"Episode {episode+1}, Average Reward: {avg_reward:.2f}\")\n","\n","    env.close()\n","\n","if __name__ == \"__main__\":\n","    train()\n"]}]}
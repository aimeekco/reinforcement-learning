{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"KJUp6QBRY-Qd"},"outputs":[],"source":[" pip install gymnasium"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K_GLg7lbayhm"},"outputs":[],"source":["from time import sleep\n","import numpy as np\n","from IPython.display import clear_output\n","import gymnasium as gym\n","from gymnasium.envs.registration import register\n","import torch\n","from torch import nn\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7PWN1PkGe66q"},"outputs":[],"source":["#Give colab access to your google drive:\n","from google.colab import drive\n","drive.mount('/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1SCX1d90YjOg"},"outputs":[],"source":["#Change current directory to folder with MiniPacMan\n","%cd /gdrive/MyDrive/SP 25/Reinforcement Learning/A2C"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GCa5TYdVWL2y"},"outputs":[],"source":["#Import MiniPacMan environment class definition\n","from MiniPacManGymV2 import MiniPacManEnv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TcY1Q97RRy6J"},"outputs":[],"source":["#Register MiniPacMan in your gymnasium environments\n","register(\n","    id=\"MiniPacMan-v0\",\n","    entry_point=MiniPacManEnv,\n","    max_episode_steps=20\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k7hwnC7Ob9VJ"},"outputs":[],"source":["#Create a MiniPacMan gymnasium environment\n","envs = gym.make_vec(\"MiniPacMan-v0\", render_mode=\"human\", frozen_ghost=False, num_envs=16, wrappers=[gym.wrappers.Autoreset])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y6irumLQsc1p"},"outputs":[],"source":["class PolicyNetwork(nn.Module):\n","    def __init__(self):\n","      super().__init__()\n","      self.linear1=nn.Linear(36,32)\n","      self.linear2=nn.Linear(32,16)\n","      self.linear3=nn.Linear(16,4)\n","\n","    def forward(self, x):\n","      x=nn.Flatten()(x)\n","      x=self.linear1(x)\n","      x=nn.ReLU()(x)\n","      x=self.linear2(x)\n","      x=nn.ReLU()(x)\n","      x=self.linear3(x)\n","      x=nn.Softmax(dim=1)(x)\n","      return x\n","\n","class ValueNetwork(nn.Module):\n","    def __init__(self):\n","      super().__init__()\n","      self.linear1=nn.Linear(36,32)\n","      self.linear2=nn.Linear(32,16)\n","      self.linear3=nn.Linear(16,1)\n","\n","    def forward(self, x):\n","      x=nn.Flatten()(x)\n","      x=self.linear1(x)\n","      x=nn.ReLU()(x)\n","      x=self.linear2(x)\n","      x=nn.ReLU()(x)\n","      x=self.linear3(x)\n","      return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2r-T_Um1XnTU"},"outputs":[],"source":["pi = PolicyNetwork()\n","V = ValueNetwork()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1mgy9tJVGAK2"},"outputs":[],"source":["pi_optimizer = torch.optim.Adam(pi.parameters(), lr=0.01)\n","V_optimizer = torch.optim.Adam(V.parameters(), lr=0.01)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0fe-YvvwKpAZ","outputId":"07839098-22fc-4fb7-c97d-6f07707c3ca2","executionInfo":{"status":"ok","timestamp":1743452832962,"user_tz":420,"elapsed":33777,"user":{"displayName":"Aimee Co","userId":"03856118716626959212"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Update 10/1000 | Policy Loss: 0.00 | Value Loss: 75.52 | Win %: 55.42%\n","Update 20/1000 | Policy Loss: 0.00 | Value Loss: 43.59 | Win %: 61.54%\n","Update 30/1000 | Policy Loss: 0.00 | Value Loss: 70.78 | Win %: 62.07%\n","Update 40/1000 | Policy Loss: 0.00 | Value Loss: 44.58 | Win %: 60.40%\n","Update 50/1000 | Policy Loss: -0.00 | Value Loss: 60.57 | Win %: 61.75%\n","Update 60/1000 | Policy Loss: -0.00 | Value Loss: 79.21 | Win %: 59.27%\n","Update 70/1000 | Policy Loss: 0.00 | Value Loss: 86.37 | Win %: 60.33%\n","Update 80/1000 | Policy Loss: 0.00 | Value Loss: 47.39 | Win %: 59.80%\n","Update 90/1000 | Policy Loss: 0.00 | Value Loss: 62.41 | Win %: 58.70%\n","Update 100/1000 | Policy Loss: 0.00 | Value Loss: 40.76 | Win %: 59.86%\n","Update 110/1000 | Policy Loss: -0.00 | Value Loss: 57.77 | Win %: 59.58%\n","Update 120/1000 | Policy Loss: -0.00 | Value Loss: 113.68 | Win %: 60.27%\n","Update 130/1000 | Policy Loss: -0.00 | Value Loss: 65.94 | Win %: 60.37%\n","Update 140/1000 | Policy Loss: 0.00 | Value Loss: 68.68 | Win %: 59.74%\n","Update 150/1000 | Policy Loss: -0.00 | Value Loss: 81.99 | Win %: 59.71%\n","Update 160/1000 | Policy Loss: -0.00 | Value Loss: 66.54 | Win %: 59.42%\n","Update 170/1000 | Policy Loss: -0.00 | Value Loss: 57.18 | Win %: 58.68%\n","Update 180/1000 | Policy Loss: -0.00 | Value Loss: 98.53 | Win %: 58.35%\n","Update 190/1000 | Policy Loss: 0.00 | Value Loss: 67.65 | Win %: 58.17%\n","Update 200/1000 | Policy Loss: -0.00 | Value Loss: 50.64 | Win %: 58.45%\n","Update 210/1000 | Policy Loss: 0.00 | Value Loss: 79.36 | Win %: 58.44%\n","Update 220/1000 | Policy Loss: -0.00 | Value Loss: 101.28 | Win %: 58.49%\n","Update 230/1000 | Policy Loss: -0.00 | Value Loss: 86.27 | Win %: 58.72%\n","Update 240/1000 | Policy Loss: -0.00 | Value Loss: 71.98 | Win %: 58.50%\n","Update 250/1000 | Policy Loss: -0.00 | Value Loss: 81.93 | Win %: 58.39%\n","Update 260/1000 | Policy Loss: -0.00 | Value Loss: 59.98 | Win %: 58.44%\n","Update 270/1000 | Policy Loss: -0.00 | Value Loss: 86.85 | Win %: 58.55%\n","Update 280/1000 | Policy Loss: 0.00 | Value Loss: 68.96 | Win %: 58.71%\n","Update 290/1000 | Policy Loss: 0.00 | Value Loss: 69.54 | Win %: 58.68%\n","Update 300/1000 | Policy Loss: 0.00 | Value Loss: 73.10 | Win %: 58.79%\n","Update 310/1000 | Policy Loss: 0.00 | Value Loss: 25.13 | Win %: 58.46%\n","Update 320/1000 | Policy Loss: 0.00 | Value Loss: 61.44 | Win %: 58.48%\n","Update 330/1000 | Policy Loss: -0.00 | Value Loss: 78.10 | Win %: 58.54%\n","Update 340/1000 | Policy Loss: 0.00 | Value Loss: 84.66 | Win %: 58.50%\n","Update 350/1000 | Policy Loss: -0.00 | Value Loss: 89.95 | Win %: 58.61%\n","Update 360/1000 | Policy Loss: 0.00 | Value Loss: 126.56 | Win %: 58.75%\n","Update 370/1000 | Policy Loss: -0.00 | Value Loss: 34.82 | Win %: 58.91%\n","Update 380/1000 | Policy Loss: 0.00 | Value Loss: 64.28 | Win %: 58.86%\n","Update 390/1000 | Policy Loss: -0.00 | Value Loss: 73.88 | Win %: 59.07%\n","Update 400/1000 | Policy Loss: -0.00 | Value Loss: 90.95 | Win %: 59.16%\n","Update 410/1000 | Policy Loss: 0.00 | Value Loss: 75.49 | Win %: 59.08%\n","Update 420/1000 | Policy Loss: -0.00 | Value Loss: 93.80 | Win %: 59.14%\n","Update 430/1000 | Policy Loss: -0.00 | Value Loss: 90.51 | Win %: 59.36%\n","Update 440/1000 | Policy Loss: -0.00 | Value Loss: 53.28 | Win %: 59.36%\n","Update 450/1000 | Policy Loss: -0.00 | Value Loss: 50.36 | Win %: 59.44%\n","Update 460/1000 | Policy Loss: -0.00 | Value Loss: 48.24 | Win %: 59.40%\n","Update 470/1000 | Policy Loss: -0.00 | Value Loss: 115.46 | Win %: 59.45%\n","Update 480/1000 | Policy Loss: 0.00 | Value Loss: 108.65 | Win %: 59.31%\n","Update 490/1000 | Policy Loss: 0.00 | Value Loss: 65.72 | Win %: 59.13%\n","Update 500/1000 | Policy Loss: -0.00 | Value Loss: 74.80 | Win %: 59.17%\n","Update 510/1000 | Policy Loss: -0.00 | Value Loss: 68.76 | Win %: 59.27%\n","Update 520/1000 | Policy Loss: 0.00 | Value Loss: 55.22 | Win %: 59.34%\n","Update 530/1000 | Policy Loss: -0.00 | Value Loss: 49.73 | Win %: 59.26%\n","Update 540/1000 | Policy Loss: -0.00 | Value Loss: 84.69 | Win %: 59.33%\n","Update 550/1000 | Policy Loss: -0.00 | Value Loss: 62.22 | Win %: 59.36%\n","Update 560/1000 | Policy Loss: 0.00 | Value Loss: 90.19 | Win %: 59.20%\n","Update 570/1000 | Policy Loss: -0.00 | Value Loss: 61.70 | Win %: 59.22%\n","Update 580/1000 | Policy Loss: 0.00 | Value Loss: 70.73 | Win %: 59.21%\n","Update 590/1000 | Policy Loss: -0.00 | Value Loss: 62.61 | Win %: 59.10%\n","Update 600/1000 | Policy Loss: 0.00 | Value Loss: 98.21 | Win %: 59.07%\n","Update 610/1000 | Policy Loss: -0.00 | Value Loss: 48.44 | Win %: 59.24%\n","Update 620/1000 | Policy Loss: -0.00 | Value Loss: 90.10 | Win %: 59.32%\n","Update 630/1000 | Policy Loss: -0.00 | Value Loss: 99.67 | Win %: 59.28%\n","Update 640/1000 | Policy Loss: 0.00 | Value Loss: 64.18 | Win %: 59.24%\n","Update 650/1000 | Policy Loss: -0.00 | Value Loss: 82.11 | Win %: 59.32%\n","Update 660/1000 | Policy Loss: -0.00 | Value Loss: 81.76 | Win %: 59.41%\n","Update 670/1000 | Policy Loss: -0.00 | Value Loss: 103.86 | Win %: 59.44%\n","Update 680/1000 | Policy Loss: -0.00 | Value Loss: 67.05 | Win %: 59.43%\n","Update 690/1000 | Policy Loss: 0.00 | Value Loss: 60.45 | Win %: 59.49%\n","Update 700/1000 | Policy Loss: -0.00 | Value Loss: 74.05 | Win %: 59.55%\n","Update 710/1000 | Policy Loss: -0.00 | Value Loss: 89.15 | Win %: 59.56%\n","Update 720/1000 | Policy Loss: -0.00 | Value Loss: 70.99 | Win %: 59.59%\n","Update 730/1000 | Policy Loss: 0.00 | Value Loss: 54.37 | Win %: 59.63%\n","Update 740/1000 | Policy Loss: -0.00 | Value Loss: 77.62 | Win %: 59.81%\n","Update 750/1000 | Policy Loss: -0.00 | Value Loss: 52.09 | Win %: 59.86%\n","Update 760/1000 | Policy Loss: -0.00 | Value Loss: 47.12 | Win %: 60.05%\n","Update 770/1000 | Policy Loss: 0.00 | Value Loss: 84.84 | Win %: 60.03%\n","Update 780/1000 | Policy Loss: -0.00 | Value Loss: 95.94 | Win %: 60.01%\n","Update 790/1000 | Policy Loss: -0.00 | Value Loss: 64.86 | Win %: 60.01%\n","Update 800/1000 | Policy Loss: -0.00 | Value Loss: 28.82 | Win %: 60.07%\n","Update 810/1000 | Policy Loss: -0.00 | Value Loss: 62.59 | Win %: 59.95%\n","Update 820/1000 | Policy Loss: -0.00 | Value Loss: 42.82 | Win %: 59.91%\n","Update 830/1000 | Policy Loss: -0.00 | Value Loss: 56.19 | Win %: 59.92%\n","Update 840/1000 | Policy Loss: -0.00 | Value Loss: 50.01 | Win %: 59.98%\n","Update 850/1000 | Policy Loss: 0.00 | Value Loss: 45.17 | Win %: 59.97%\n","Update 860/1000 | Policy Loss: 0.00 | Value Loss: 69.40 | Win %: 59.96%\n","Update 870/1000 | Policy Loss: -0.00 | Value Loss: 57.89 | Win %: 59.99%\n","Update 880/1000 | Policy Loss: 0.00 | Value Loss: 48.15 | Win %: 59.95%\n","Update 890/1000 | Policy Loss: -0.00 | Value Loss: 27.40 | Win %: 59.85%\n","Update 900/1000 | Policy Loss: -0.00 | Value Loss: 62.99 | Win %: 59.70%\n","Update 910/1000 | Policy Loss: -0.00 | Value Loss: 44.67 | Win %: 59.68%\n","Update 920/1000 | Policy Loss: 0.00 | Value Loss: 104.78 | Win %: 59.71%\n","Update 930/1000 | Policy Loss: -0.00 | Value Loss: 91.99 | Win %: 59.66%\n","Update 940/1000 | Policy Loss: -0.00 | Value Loss: 67.93 | Win %: 59.76%\n","Update 950/1000 | Policy Loss: 0.00 | Value Loss: 42.92 | Win %: 59.79%\n","Update 960/1000 | Policy Loss: 0.00 | Value Loss: 74.08 | Win %: 59.86%\n","Update 970/1000 | Policy Loss: -0.00 | Value Loss: 92.87 | Win %: 59.89%\n","Update 980/1000 | Policy Loss: 0.00 | Value Loss: 56.92 | Win %: 59.94%\n","Update 990/1000 | Policy Loss: -0.00 | Value Loss: 42.38 | Win %: 60.03%\n","Update 1000/1000 | Policy Loss: -0.00 | Value Loss: 62.82 | Win %: 60.01%\n"]}],"source":["from math import log\n","\n","#set hyperparams\n","gamma=0.99\n","num_updates=1000\n","num_trajectories_parallel=16\n","num_steps = 7\n","\n","done_log = []\n","reward_log = []\n","\n","win_pct=np.zeros(num_updates)\n","episode_rewards = [[] for _ in range(num_trajectories_parallel)]\n","current_rewards = np.zeros(num_trajectories_parallel)\n","\n","new_obs, info = envs.reset()\n","\n","for e in range(num_updates):\n","\n","    states_list = []\n","    log_probs_list = []\n","    rewards_list = []\n","    next_states_list = []\n","    dones_list = []\n","\n","    obs = torch.as_tensor(new_obs,dtype=torch.float32)\n","\n","    for k in range(num_steps):\n","        probs = pi(obs)\n","        actions=probs.multinomial(num_samples=1).squeeze(-1)\n","        action_probs = probs.gather(1, actions.unsqueeze(1)).squeeze(1)\n","        log_probs = torch.log(action_probs)\n","\n","        #step in all parallel trajectories\n","        new_obs,rewards, dones, truncated, infos = envs.step(actions.cpu().numpy())\n","        new_obs = torch.as_tensor(new_obs,dtype=torch.float32)\n","\n","        #update rewards\n","        current_rewards += np.array(rewards)\n","\n","        for i in range(num_trajectories_parallel):\n","            if dones[i]:\n","                episode_rewards[i].append(current_rewards[i])\n","                current_rewards[i] = 0.0\n","\n","        #append current state to done and reward logs\n","        done_log.extend(dones)\n","        reward_log.extend(rewards)\n","\n","        # add (s, pi(s, a), r, s', done) to respective arrays\n","        states_list.append(obs)\n","        log_probs_list.append(log_probs)\n","        rewards_list.append(torch.tensor(rewards, dtype=torch.float32))\n","        next_states_list.append(new_obs.clone().detach())\n","        dones_list.append(torch.tensor(dones, dtype=torch.float32))\n","\n","        obs=new_obs\n","\n","    rewards_t = torch.stack(rewards_list)\n","    states_t = torch.stack(states_list)\n","    log_probs_t = torch.stack(log_probs_list)\n","    dones_t = torch.stack(dones_list)\n","\n","    #use final next states from last step\n","    final_next_states = next_states_list[-1]\n","    final_next_states_flat = final_next_states.view(final_next_states.size(0), -1)\n","    with torch.no_grad():\n","        v_final = V(final_next_states_flat).squeeze(-1)\n","\n","    #buffer for n step targets\n","    targets = torch.zeros_like(rewards_t)\n","\n","    targets[-1] = rewards_t[-1] + (1-dones_t[-1])*gamma*v_final\n","\n","    for t in reversed(range(num_steps - 1)):\n","        targets[t] = rewards_t[t] + (1-dones_t[t])*gamma*targets[t+1]\n","\n","    num_transitions=num_steps * num_trajectories_parallel\n","    targets_flat = targets.view(num_transitions)\n","    states_flat = states_t.view(num_transitions, -1)\n","    log_probs_flat = log_probs_t.view(num_transitions)\n","\n","    #for t in reversed(range(5)):\n","        #for i in range(num_trajectories_parallel):\n","            #idx = t * num_trajectories_parallel + i\n","\n","            #if t == 4:\n","            ##r_t + (1 - done_t)*gamma*V(next_state)\n","                #targets[idx] = rewards_t[idx] + (1 - dones_t[idx]) * gamma * v_next[idx]\n","            #else:\n","            ##r_t + (1 - done_t)*gamma*targets[t+1]\n","                #idx_next = (t + 1) * num_trajectories_parallel + i\n","                #targets[idx] = rewards_t[idx] + (1 - dones_t[idx]) * gamma * targets[idx_next]\n","\n","    #compute value loss\n","    v_s = V(states_flat).squeeze(-1)\n","    advantages = targets_flat - v_s\n","    loss_V = (advantages ** 2).mean()\n","\n","    V_optimizer.zero_grad()\n","    loss_V.backward()\n","    V_optimizer.step()\n","\n","    with torch.no_grad():\n","        v_s = V(states_flat).squeeze(-1)\n","        advantages = targets_flat - v_s\n","\n","    loss_pi = -(log_probs_flat * advantages.detach()).mean()\n","\n","    pi_optimizer.zero_grad()\n","    loss_pi.backward()\n","    pi_optimizer.step()\n","\n","\n","\n","    #periodic reporting:\n","    done_log_np = np.array(done_log)\n","    reward_log_np = np.array(reward_log)\n","\n","    terminal_rewards = reward_log_np[done_log_np.astype(bool)]\n","    win_pct = np.mean(terminal_rewards == 20) * 100\n","    # win_pct[e] = np.mean((reward_log_np[done_log_np] == 20)[:-100])\n","\n","    if (e + 1) % 10 == 0:\n","      print(f\"Update {e+1}/{num_updates} | Policy Loss: {loss_pi.item():.2f} | Value Loss: {loss_V.item():.2f} | Win %: {win_pct:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C33zxXrdGh_y","colab":{"base_uri":"https://localhost:8080/","height":326},"executionInfo":{"status":"error","timestamp":1743452791645,"user_tz":420,"elapsed":17278,"user":{"displayName":"Aimee Co","userId":"03856118716626959212"}},"outputId":"50a03357-1256-4ff9-cf09-ab376c047c60"},"outputs":[{"output_type":"stream","name":"stdout","text":["xxxxxx\n","x····x\n","xᗧ··ᗣx\n","x····x\n","x···◯x\n","xxxxxx\n","\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-41fceb9989fc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["obs, info = envs.reset()\n","num_envs = obs.shape[0]\n","dones = np.zeros(num_envs, dtype=bool)\n","truncateds = np.zeros(num_envs, dtype=bool)\n","\n","while not np.all(dones) and not np.all(truncateds):\n","    envs.envs[0].render()\n","    obs = torch.tensor(obs, dtype=torch.float32)\n","    actions = pi(obs).multinomial(num_samples=1).squeeze(-1)\n","    obs, rewards, dones, truncated, infos = envs.step(actions.cpu().numpy())\n","\n","    sleep(1)\n","    clear_output(wait=True)\n","\n","envs.envs[0].render()\n","envs.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZjO1S_URYs2","executionInfo":{"status":"error","timestamp":1743489287739,"user_tz":420,"elapsed":52,"user":{"displayName":"Aimee Co","userId":"03856118716626959212"}},"outputId":"f1dae4b6-c55b-45af-8d48-d21b2f0e91df","colab":{"base_uri":"https://localhost:8080/","height":156}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'win_pct' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-32b53df3e342>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwin_pct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'win_pct' is not defined"]}],"source":["from matplotlib.pyplot import plot\n","plot(win_pct)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IMNcW3GGI6Pe"},"outputs":[],"source":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}